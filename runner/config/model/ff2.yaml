model_name: "ff2"
esm2_model_key: "esm2_650M" # Trained with "esm2_650M"
scaffold_training: False
binder_training: False
binder_percent_fix_structure: 1.0
bb_encoder:
  num_blocks: 4 # 2
  coordinate_scaling: ${flow_matcher.r3.coordinate_scaling}
bb_decoder:
  num_blocks: 2
  coordinate_scaling: ${flow_matcher.r3.coordinate_scaling}
seq_emb_to_block:
  single_dim: 128 
  pair_dim: 128
bb_mace_encoder_to_block:
  single_dim: 256
representation_combiner:
  single_dim: 128 # NOTE: If proj+concat, the total dim will be 256 or 384 (if using MACE)
  pair_dim: 64 # NOTE: If proj+concat, the total dim will be 128
  layer_norm: True
modalities_transformer:
  trunk_type: "transformer"
  num_blocks: 2
  sequence_head_width: 32
  pairwise_head_width: 32
  chunk_size: null # null won't chunk. Lower chunk_size reduce memory, but reduces speed.
p_mask_sequence: 0.5

embed:
  embed_self_conditioning: True
  use_alphafold_position_embedding: False
  relpos_k: null

mace_encoder:
  is_on: False
  num_layers: 5
  emb_dim: 128
  mlp_dim: 256
  encoder_dim: 256
  in_dim: 1 # take only 1 type of atoms --> C-alpha backbone
  out_dim: 1 # dummy var, not used, since MACE is used only as encoder
  aggr: "sum"
  pool: "sum"
  batch_norm: True 

graph:
  knn_k: 10
  knn_min_dist: 5
  knn_max_dist: null
  radius_r: 5
  radius_min_dist: 5
  radius_max_dist: null
  radius_max_num_neighbors: 10
  max_squared_res_ratio: 0.3
