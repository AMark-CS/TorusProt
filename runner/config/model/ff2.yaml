model_name: "ff2"
esm2_model_key: "esm2_650M" # Trained with "esm2_650M"
scaffold_training: False
binder_training: False
binder_percent_fix_structure: 1.0
bb_encoder:
  num_blocks: 4 # 2
  coordinate_scaling: ${flow_matcher.r3.coordinate_scaling}
bb_decoder:
  num_blocks: 2
  coordinate_scaling: ${flow_matcher.r3.coordinate_scaling}
seq_emb_to_block:
  single_dim: 128 
  pair_dim: 128
representation_combiner:
  single_dim: 128 # NOTE: If proj+concat, the total dim will be 512
  pair_dim: 64 # NOTE: If proj+concat, the total dim will be 512
  layer_norm: True
modalities_transformer:
  trunk_type: "transformer"
  num_blocks: 2
  sequence_head_width: 32
  pairwise_head_width: 32
  chunk_size: null # null won't chunk. Lower chunk_size reduce memory, but reduces speed.
p_mask_sequence: 0.5

embed:
  embed_self_conditioning: True
  use_alphafold_position_embedding: False
  relpos_k: null

mace:
  num_layers: 5
  emb_dim: 128
  mlp_dim: 256
  in_dim: 37 # 37 possible atom types
  out_dim: 1 # not used, since MACE is used only as encoder
  aggr: "sum"
  pool: "sum"
  batch_norm: False # set to False for small batch size