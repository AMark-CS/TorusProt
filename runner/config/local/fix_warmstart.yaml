# @package _global_

# Add your local paths here.
# for the paths for the datasets, the ckpt, wandb, etc.

data:
  csv_path: ./data/metadata_one.csv
  cluster_path: ./data/clusters-by-entity-30.txt # or any cluster file
  filtering:
    max_len: 300
    min_len: 60
  min_t: 0.01
  cache_full_dataset: False  # Cache both to disk (LMDB) and in memory.
  cache_dataset_in_memory: False # If True load from mem. If False, load from disk (LMDB).
  cache_path: ./cache/  # Where to save the LMDB cache.
  samples_per_eval_length: 4
  num_eval_lengths: 10

model:
  mace_encoder:
    is_on: False
    max_ell: 1
    num_layers: 5
    max_squared_res_ratio: 0.2
    emb_dim: 128
    mlp_dim: 128

experiment:
  # Experiment metadata
  name: fix_warmstart
  run_id: null

  # Training mode
  use_ddp: False
  debug: False

  # Warm start configuration
  warm_start: null
  use_warm_start_conf: False

  # Training arguments
  log_freq: 10
  batch_size: 1
  eval_batch_size: ${data.samples_per_eval_length}
  num_loader_workers: 1
  torch_num_threads: 1
  num_epoch: 300
  learning_rate: 0.0001
  max_squared_res: 500000
  prefetch_factor: 100
  use_gpu: True
  num_gpus: 1
  sample_mode: cluster_time_batch

  # How many steps to checkpoint between.
  ckpt_freq: 100
  eval_freq: 100
  # Take early checkpoint at step 100. Helpful for catching eval bugs early.
  early_ckpt: True
  ckpt_dir: ./ckpt_dir

  # Weights and Biases log dir
  wandb_dir: ./wandb/

flow_matcher:
  stochastic_paths: False

